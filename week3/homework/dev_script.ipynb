{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71222a-0aa9-46f7-87ee-9fc9c3eba3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7692d4ae-3b40-4559-b93b-097fde7724b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Data_Engineering_Zoomcamp/week3/homework/.venv/lib/python3.12/site-packages/google/auth/_default.py:114: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Bucket created: ingest-test-jianpeng-123\n",
      "Location: US\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# 1. Setup (Replace with your specific Project ID)\n",
    "project_id = \"avid-circle-484315-e5\" \n",
    "client = storage.Client(project=project_id)\n",
    "\n",
    "# 2. Pick a name (MUST BE GLOBALLY UNIQUE)\n",
    "bucket_name = \"ingest-test-jianpeng-123\" # <--- CHANGE THIS\n",
    "\n",
    "# 3. Create it\n",
    "try:\n",
    "    # We create the bucket reference\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    # We send the \"create\" command to Google\n",
    "    # location=\"US\" is standard. Use \"EU\" if you are in Europe.\n",
    "    new_bucket = client.create_bucket(bucket, location=\"US\")\n",
    "    \n",
    "    print(f\"✅ Success! Bucket created: {new_bucket.name}\")\n",
    "    print(f\"Location: {new_bucket.location}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ Creation failed.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c6e843-83dc-409e-8375-346eeeb80ccc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Generating 'Hello World' data...\n",
      "2. Uploading to Bucket: ingest-test-jianpeng-123...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Data_Engineering_Zoomcamp/week3/homework/.venv/lib/python3.12/site-packages/google/auth/_default.py:114: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Upload complete.\n",
      "3. Instructing BigQuery to ingest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Data_Engineering_Zoomcamp/week3/homework/.venv/lib/python3.12/site-packages/google/auth/_default.py:114: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Created new dataset: hello_world_dataset\n",
      "   ⏳ Job running...\n",
      "   ✅ Job finished! Loaded 3 rows into hello_world_dataset.my_first_table\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import io\n",
    "\n",
    "# --- CONFIGURATION (UPDATE THESE!) ---\n",
    "PROJECT_ID = \"avid-circle-484315-e5\"\n",
    "BUCKET_NAME = \"ingest-test-jianpeng-123\" # The one you just created\n",
    "DATASET_NAME = \"hello_world_dataset\"        # We will create this inside BQ\n",
    "\n",
    "# --- STEP 1: GENERATE DATA ---\n",
    "print(\"1. Generating 'Hello World' data...\")\n",
    "data = [\n",
    "    {\"id\": 1, \"message\": \"Hello World\", \"status\": \"Success\"},\n",
    "    {\"id\": 2, \"message\": \"This is a test\", \"status\": \"Pending\"},\n",
    "    {\"id\": 3, \"message\": \"BigQuery is cool\", \"status\": \"Done\"}\n",
    "]\n",
    "df = pd.read_json(io.StringIO(str(data).replace(\"'\", '\"')))\n",
    "\n",
    "# --- STEP 2: UPLOAD TO BUCKET ---\n",
    "print(f\"2. Uploading to Bucket: {BUCKET_NAME}...\")\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "blob = bucket.blob(\"hello_world.parquet\")\n",
    "\n",
    "# Save DataFrame to Parquet format in memory, then upload\n",
    "blob.upload_from_string(df.to_parquet(), content_type=\"application/octet-stream\")\n",
    "print(\"   ✅ Upload complete.\")\n",
    "\n",
    "# --- STEP 3: LOAD INTO BIGQUERY ---\n",
    "print(\"3. Instructing BigQuery to ingest...\")\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Create the Dataset if it doesn't exist\n",
    "dataset_ref = bq_client.dataset(DATASET_NAME)\n",
    "try:\n",
    "    bq_client.get_dataset(dataset_ref)\n",
    "    print(f\"   Dataset {DATASET_NAME} already exists.\")\n",
    "except Exception:\n",
    "    bq_client.create_dataset(dataset_ref)\n",
    "    print(f\"   Created new dataset: {DATASET_NAME}\")\n",
    "\n",
    "# Configure the Load Job\n",
    "table_ref = dataset_ref.table(\"my_first_table\")\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.PARQUET,\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE, # Overwrite table if exists\n",
    ")\n",
    "\n",
    "# The Magic Command: Load from URI (Not from script memory!)\n",
    "uri = f\"gs://{BUCKET_NAME}/hello_world.parquet\"\n",
    "load_job = bq_client.load_table_from_uri(uri, table_ref, job_config=job_config)\n",
    "\n",
    "print(\"   ⏳ Job running...\")\n",
    "load_job.result() # Waits for the job to complete\n",
    "\n",
    "print(f\"   ✅ Job finished! Loaded {load_job.output_rows} rows into {DATASET_NAME}.my_first_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67ce00fd-f8b8-4d35-aabd-08ab1afd61fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket data-engineering-zoomcamp-jianpeng-20260122 exists.\n",
      "2. Uploading to Bucket data-engineering-zoomcamp-jianpeng-20260122 ...\n",
      "Starting stream from url\n",
      "   ✅ Upload complete.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.cloud import storage\n",
    "import requests\n",
    "\n",
    "#config\n",
    "BUCKET_NAME = \"data-engineering-zoomcamp-jianpeng-20260122\"\n",
    "PARQUET_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "PROJECT_ID = \"avid-circle-484315-e5\"\n",
    "\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "#step 1, try to get the bucket, or create it if it doesn't exist\n",
    "try:\n",
    "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "    print(f\"Bucket {BUCKET_NAME} exists.\")\n",
    "except NotFound:\n",
    "    print(f\"Bucket {BUCKET_NAME} not found. Creating it...\")\n",
    "    # Note: location=\"US\" (or \"EU\", \"ASIA\") is important for performance/compliance\n",
    "    bucket = storage_client.create_bucket(BUCKET_NAME, location=\"US\")\n",
    "    print(f\"Bucket {BUCKET_NAME} created successfully.\")\n",
    "\n",
    "print(f\"2. Uploading to Bucket {BUCKET_NAME} ...\")\n",
    "blob = bucket.blob(\"hello_world.parquet\")\n",
    "\n",
    "#upload form parquet url\n",
    "print(f\"Starting stream from url\")\n",
    "with requests.get(PARQUET_URL, stream = True) as response:\n",
    "    response.raise_for_status()\n",
    "    response.raw.decode_content= True\n",
    "    blob.upload_from_file(response.raw)\n",
    "    print(\"   ✅ Upload complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "972002e7-59bc-491c-97f8-8e8a287f24cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Instructing BigQuery to ingest...\n",
      "   Created new dataset: ny_taxi\n",
      "   ⏳ Job running...\n",
      "   ✅ Job finished! Loaded 2964624 rows into ny_taxi.my_first_table\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 3: LOAD INTO BIGQUERY ---\n",
    "DATASET_NAME= \"ny_taxi\"\n",
    "\n",
    "print(\"3. Instructing BigQuery to ingest...\")\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Create the Dataset if it doesn't exist\n",
    "dataset_ref = bq_client.dataset(DATASET_NAME)\n",
    "try:\n",
    "    bq_client.get_dataset(dataset_ref)\n",
    "    print(f\"   Dataset {DATASET_NAME} already exists.\")\n",
    "except Exception:\n",
    "    bq_client.create_dataset(dataset_ref)\n",
    "    print(f\"   Created new dataset: {DATASET_NAME}\")\n",
    "\n",
    "# Configure the Load Job\n",
    "table_ref = dataset_ref.table(\"yellow_taxi_data\")\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.PARQUET,\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE, # Overwrite table if exists\n",
    ")\n",
    "\n",
    "# The Magic Command: Load from URI (Not from script memory!)\n",
    "uri = f\"gs://{BUCKET_NAME}/hello_world.parquet\"\n",
    "load_job = bq_client.load_table_from_uri(uri, table_ref, job_config=job_config)\n",
    "\n",
    "print(\"   ⏳ Job running...\")\n",
    "load_job.result() # Waits for the job to complete\n",
    "\n",
    "print(f\"   ✅ Job finished! Loaded {load_job.output_rows} rows into {DATASET_NAME}.my_first_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357db25-8569-4c1d-a7a5-ce2accef2aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# 3.3 Export and Serving (MLOps)

**[↑ Up](README.md)** | **[← Previous](3.2-bqml-training.md)** | **[Next →](3.4-prediction-test.md)**

---

**Folder Structure:**
```
project/
├── 3.1-optimization.md
├── 3.2-bqml-training.md
├── 3.3-export-serving.md  <-- You are here
└── 3.4-prediction-test.md
```

## Objective

Now that we have a trained model, we want to take it out of the data warehouse and deploy it as a microservice. We will **export** the model to Google Cloud Storage (GCS) and then serve it locally using **TensorFlow Serving**.

## 1. Prepare Google Cloud Environment

We need to use the `gcloud` command-line tool.

### Step 1: open Terminal
Open your terminal application on your computer.

### Step 2: Authenticate
Login to your Google Cloud account:

```bash
gcloud auth login
```

*   A browser window will open.
*   Log in with your Google credentials.
*   Allow access to the Google Cloud SDK.

Set your project ID:

```bash
# Replace YOUR_PROJECT_ID with your actual project ID
gcloud config set project YOUR_PROJECT_ID
```

## 2. Export Model to GCS

We first export the model from BigQuery to a Google Cloud Storage (GCS) bucket.

### Step 3: Create a Bucket
Buckets are like folders in the cloud. You need a unique name.

```bash
# Replace 'your-unique-bucket-name' with something unique (e.g., yourname-taxi-model)
gsutil mb gs://your-unique-bucket-name
```

### Step 4: Export the Model
Use the `bq` command to extract the model to your bucket.

```bash
# Syntax: bq extract -m <dataset>.<model> <destination>
bq extract -m your_dataset.tip_model gs://your-unique-bucket-name/tip_model
```

*   **Note**: Make sure `your_dataset` matches the dataset name you created in step 3.1.

## 3. Download Model Locally

Now we bring those files to our local machine to prepare them for Docker.

### Step 5: specific Directory Pattern
TensorFlow Serving requires a specific folder structure: `model_name/version_number`.

Run these commands in your terminal:

```bash
# 1. Create the directory structure (e.g., serving_dir/tip_model/1)
mkdir -p serving_dir/tip_model/1
```

### Step 6: Copy Files
Download the files from GCS to your local folder `1`.

```bash
# 2. Copy files from GCS to the version folder
gsutil cp -r gs://your-unique-bucket-name/tip_model/* serving_dir/tip_model/1/
```

*   **Verify**: Run `ls -R serving_dir` to ensure files are inside the `1` folder.

## 4. Serving with Docker

We will use the official TensorFlow Serving image to host our model as an HTTP endpoint.

### Step 7: Run the Container
Run the following command. It will start a server on your machine on port 8501.

```bash
docker run -t --rm \
  -p 8501:8501 \
  -v "$(pwd)/serving_dir/tip_model:/models/tip_model" \
  -e MODEL_NAME=tip_model \
  tensorflow/serving
```

**Explanation:**
*   **`-p 8501:8501`**: Connects port 8501 on your computer to the container.
*   **`-v ...`**: Maps your local `serving_dir/tip_model` folder to the container's `/models/tip_model` folder.
*   **`-e MODEL_NAME=tip_model`**: Tells the server which model to load.

**Success Message:**
You should see log output ending with:
`Exporting HTTP/REST API at:NaN:8501 ...`

**Do not close this terminal.** Open a NEW terminal tab or window for the next step.
